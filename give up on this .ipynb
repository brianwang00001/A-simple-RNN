{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## difference: (mini) batch learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "olivia\n",
      "ava\n",
      "isabella\n",
      "sophia\n",
      "charlotte\n",
      "mia\n",
      "amelia\n",
      "harper\n",
      "evelyn\n",
      "abigail\n",
      "emily\n",
      "elizabeth\n",
      "mila\n",
      "ella\n",
      "Data has 228144 characters, 27 unique.\n",
      "One epoch = 11407.200 iterations\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RNN():\n",
    "\n",
    "    def __init__(self, data, hidden_size=5, seq_length=20, reg=0):\n",
    "        self.data = data\n",
    "        #self.data = self.data[:300]\n",
    "        \n",
    "        chars = sorted(list(set(self.data)))\n",
    "        self.data_size, self.vocab_size = len(self.data)-1, len(chars)\n",
    "        self.stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i:ch for i, ch in enumerate(chars)}\n",
    "        print('Data has %d characters, %d unique.' % (self.data_size, self.vocab_size))\n",
    "\n",
    "        # regularisaiton term\n",
    "        self.reg = reg        \n",
    "\n",
    "        # transform char data into one hot encoding \n",
    "        x_index = [self.stoi[i] for i in self.data[:-1]]\n",
    "        self.X = self.one_hot(x_index)\n",
    "\n",
    "        # correct label of X\n",
    "        self.label = np.array([self.stoi[i] for i in self.data[1:]])\n",
    "\n",
    "        # hyperparameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        print(f'One epoch = {self.data_size / self.seq_length:.3f} iterations')\n",
    "        print('='*50)\n",
    "\n",
    "        # params\n",
    "        self.Wxh = np.random.randn(self.vocab_size, self.hidden_size) * 0.01\n",
    "        self.Whh = np.random.randn(self.hidden_size, self.hidden_size) * 0.01\n",
    "        self.Why = np.random.randn(self.hidden_size, self.vocab_size) * 0.01\n",
    "        self.bh = np.zeros((1, self.hidden_size))\n",
    "        self.by = np.zeros((1, self.vocab_size))\n",
    "\n",
    "        # record total training iteration \n",
    "        self.total_iteration = 0\n",
    "\n",
    "        # loss record\n",
    "        self.iter_record = []\n",
    "        self.loss_record = []\n",
    "\n",
    "        # update params(RMSprop)\n",
    "        self.mWxh = np.zeros_like(self.Wxh)\n",
    "        self.mWhh = np.zeros_like(self.Whh) \n",
    "        self.mWhy = np.zeros_like(self.Why) \n",
    "        self.mbh = np.zeros_like(self.bh) \n",
    "        self.mby = np.zeros_like(self.by)\n",
    "\n",
    "    def one_hot(self, idx):\n",
    "        size = self.vocab_size\n",
    "        out = np.zeros((len(idx), size))\n",
    "        out[np.arange(len(idx)), idx] = 1\n",
    "        return out\n",
    "\n",
    "    def softmax(self, x):\n",
    "        x -= np.max(x)\n",
    "        out = np.exp(x)\n",
    "        out = out / np.sum(out, axis=1, keepdims=True)\n",
    "        return out\n",
    "\n",
    "    # forward one step\n",
    "    def forward(self, x, h0):\n",
    "        h = np.tanh(x @ self.Wxh + h0 @ self.Whh + self.bh)\n",
    "        #h = np.maximum(0, x @ self.Wxh + h0 @ self.Whh + self.bh)\n",
    "        y = h @ self.Why + self.by\n",
    "        return h, y\n",
    "    \n",
    "    # backward on step\n",
    "    def backward(self, hprev, h, x, y, dhnext, label):\n",
    "        dy = self.softmax(np.copy(y))\n",
    "        dy[np.arange(y.shape[0]), label] -= 1\n",
    "        dby = np.mean(dy, axis=0, keepdims=True)\n",
    "        dWhy = h.T @ dy\n",
    "        dh = dy @ self.Why.T + dhnext\n",
    "        dhraw = (1 - 2*h*h) * dh\n",
    "\n",
    "        #dhraw = dh\n",
    "        #dhraw[h < 0] = 0\n",
    "\n",
    "        dbh = np.mean(dhraw, axis=0, keepdims=True)\n",
    "        dWhh = hprev.T @ dhraw\n",
    "        dWxh = x.T @ dhraw\n",
    "        dhprev = dhraw @ self.Whh.T\n",
    "        return dWhh, dWxh, dWhy, dbh, dby, dhprev\n",
    "    \n",
    "    # loss for one step\n",
    "    def cross_entropy(self, y, label):\n",
    "        out = -np.mean(np.log(self.softmax(y)[np.arange(y.shape[0]), label]))\n",
    "        return out\n",
    "\n",
    "    # train the model for 1 epoch\n",
    "    def train(self, learning_rate=1e-2, show_loss_every = 1000):\n",
    "        # set learning rate \n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        grad_threshold = 20\n",
    "        \n",
    "        hs = {}\n",
    "        hs[-1] = np.zeros((1, self.hidden_size))\n",
    "\n",
    "        for i in range(int(self.data_size / self.seq_length)):\n",
    "            loss = 0\n",
    "            \n",
    "            # forward pass for many times\n",
    "            ys = {}\n",
    "            for j in range(self.seq_length):\n",
    "                x = self.X[i*self.seq_length+j:i*self.seq_length+j+1]\n",
    "                label = self.label[i*self.seq_length+j:i*self.seq_length+j+1]\n",
    "                hs[j], ys[j] = self.forward(x, hs[j-1])\n",
    "                loss += self.cross_entropy(ys[j], label)\n",
    "                #print('forward data :', i * self.seq_length + j)\n",
    "                #print('loss accum')\n",
    "            \n",
    "            hs[-1] = hs[self.seq_length - 1] # save final hidden state\n",
    "            \n",
    "            # backward pass for many times\n",
    "            dWhh, dWxh, dWhy, dbh, dby = np.zeros_like(self.Whh), np.zeros_like(self.Wxh), np.zeros_like(self.Why), np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "            dh = np.zeros((1, self.hidden_size))\n",
    "            for j in reversed(range(self.seq_length)):\n",
    "                x = self.X[i*self.seq_length+j:i*self.seq_length+j+1]\n",
    "                label = self.label[i*self.seq_length+j:i*self.seq_length+j+1]\n",
    "                dWhh_incre, dWxh_incre, dWhy_incre, dbh_incre, dby_incre, dh_incre = self.backward(hs[j-1], hs[j], x, ys[j], dh, label)\n",
    "                for dparam, dparam_incre in zip([dWhh, dWxh, dWhy, dbh, dby, dh], \n",
    "                                            [dWhh_incre, dWxh_incre, dWhy_incre, dbh_incre, dby_incre, dh_incre]):\n",
    "                    dparam += dparam_incre\n",
    "                #print('backward pass :', i * self.seq_length + j)\n",
    "                #print('grad accum')\n",
    "\n",
    "            # update\n",
    "            decay_rate = 0.99\n",
    "            for param, dparam, mem in zip([self.Whh, self.Wxh, self.Why, self.bh, self.by],\n",
    "                                    [dWhh, dWxh, dWhy, dbh, dby],\n",
    "                                    [self.mWhh, self.mWxh, self.mWhy, self.mbh, self.mby]):\n",
    "                # regularisation\n",
    "                dparam += self.reg * param\n",
    "                # gradient clipping \n",
    "                dparam = np.clip(dparam, -grad_threshold, grad_threshold, out=dparam) \n",
    "                mem = decay_rate*mem + (1-decay_rate)*dparam*dparam\n",
    "                param += -learning_rate * dparam  / np.sqrt(mem + 1e-8)\n",
    "            \n",
    "            if self.total_iteration % show_loss_every == 0:\n",
    "                print(f'Iteration : {self.total_iteration} | Loss : {loss/self.seq_length}')\n",
    "                # record loss\n",
    "                self.iter_record.append(self.total_iteration / (self.data_size / self.seq_length))\n",
    "                self.loss_record.append(loss/self.seq_length)\n",
    "            self.total_iteration += 1\n",
    "\n",
    "            #print('update')\n",
    "            #print('-'*40)\n",
    "\n",
    "        # forward pass for the remaining times\n",
    "        ys = {}\n",
    "        for i in range(self.data_size % self.seq_length):\n",
    "            x = self.X[i+self.data_size-self.data_size%self.seq_length:i+self.data_size-self.data_size%self.seq_length+1]\n",
    "            label = self.label[i+self.data_size-self.data_size%self.seq_length:i+self.data_size-self.data_size%self.seq_length+1]\n",
    "            hs[j], ys[j] = self.forward(x, hs[j-1])\n",
    "            loss += self.cross_entropy(ys[j], label)\n",
    "            #print('forward data :', i + self.data_size - self.data_size % self.seq_length)\n",
    "            #print('loss accum')\n",
    "        hs[-1] = hs[self.seq_length - 1] # save final hidden state\n",
    "\n",
    "        # backward pass for the remaining times\n",
    "        dWhh, dWxh, dWhy, dbh, dby = np.zeros_like(self.Whh), np.zeros_like(self.Wxh), np.zeros_like(self.Why), np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        dh = np.zeros((1, self.hidden_size))\n",
    "        for i in reversed(range(self.data_size % self.seq_length)):\n",
    "            x = self.X[i+self.data_size-self.data_size%self.seq_length:i+self.data_size-self.data_size%self.seq_length+1]\n",
    "            label = self.label[i+self.data_size-self.data_size%self.seq_length:i+self.data_size-self.data_size%self.seq_length+1]\n",
    "            dWhh_incre, dWxh_incre, dWhy_incre, dbh_incre, dby_incre, dh_incre = self.backward(hs[j-1], hs[j], x, ys[j], dh, label)\n",
    "            for dparam, dparam_incre in zip([dWhh, dWxh, dWhy, dbh, dby, dh], \n",
    "                                        [dWhh_incre, dWxh_incre, dWhy_incre, dbh_incre, dby_incre, dh_incre]):\n",
    "                dparam += dparam_incre\n",
    "            #print('backward pass :', i + self.data_size - self.data_size % self.seq_length)\n",
    "            #print('grad accum')\n",
    "\n",
    "        # update (RMSprop)\n",
    "        decay_rate = 0.99\n",
    "        for param, dparam, mem in zip([self.Whh, self.Wxh, self.Why, self.bh, self.by],\n",
    "                                [dWhh, dWxh, dWhy, dbh, dby],\n",
    "                                [self.mWhh, self.mWxh, self.mWhy, self.mbh, self.mby]):\n",
    "            # regularisation\n",
    "            dparam += self.reg * param\n",
    "            # gradient clipping \n",
    "            dparam = np.clip(dparam, -grad_threshold, grad_threshold, out=dparam) \n",
    "            mem = decay_rate*mem + (1-decay_rate)*dparam*dparam\n",
    "            param += -learning_rate * dparam  / np.sqrt(mem + 1e-8)\n",
    "        #print('update')\n",
    "        #print('==end=='*40)\n",
    "\n",
    "    # sample from the model\n",
    "    def sample(self, sample_num):\n",
    "        h = np.zeros((1, self.hidden_size))\n",
    "        x = np.zeros((1, self.vocab_size))\n",
    "        sample_seed = np.random.choice(self.vocab_size, 1).item()\n",
    "        x[0, sample_seed] = 1\n",
    "        result_key = [sample_seed]\n",
    "        for i in range(sample_num):\n",
    "            h, y = self.forward(x, h)\n",
    "            probs = self.softmax(y).ravel()\n",
    "            gen_key = np.random.choice(self.vocab_size, 1, p=probs).item()\n",
    "            x = np.zeros_like(x)\n",
    "            x[0, gen_key] = 1\n",
    "            result_key.append(gen_key)\n",
    "\n",
    "        result = [self.itos[i] for i in result_key]\n",
    "        result = ''.join(result)\n",
    "        return result\n",
    "   \n",
    "# data I/O\n",
    "#data = open('atomic_habit.txt', 'r').read()\n",
    "#data = open('hp_10pages.txt', 'r').read()\n",
    "data = open('names.txt', 'r').read()\n",
    "#data = data[:4]\n",
    "print(data[:100])\n",
    "\n",
    "\n",
    "\n",
    "model2 = RNN(data, hidden_size=200, seq_length=20, reg=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 5400 | Loss : 3.2459778334357177\n",
      "Iteration : 5600 | Loss : 3.568445522625175\n",
      "Iteration : 5800 | Loss : 4.129577673206861\n",
      "Iteration : 6000 | Loss : 3.455051514264558\n",
      "Iteration : 6200 | Loss : 4.135822291331136\n",
      "Iteration : 6400 | Loss : 3.4084773279775136\n",
      "Iteration : 6600 | Loss : 2.987566285228448\n",
      "Iteration : 6800 | Loss : 2.611514062212413\n",
      "Iteration : 7000 | Loss : 2.8281191193828947\n",
      "Iteration : 7200 | Loss : 3.002393108954814\n",
      "Iteration : 7400 | Loss : 3.3455461600793983\n",
      "Iteration : 7600 | Loss : 4.230661568915121\n",
      "Iteration : 7800 | Loss : 4.39641318648941\n",
      "Iteration : 8000 | Loss : 3.1575225267479565\n",
      "Iteration : 8200 | Loss : 3.305931498520917\n",
      "Iteration : 8400 | Loss : 2.6879763355812814\n",
      "Iteration : 8600 | Loss : 4.931210203917174\n",
      "Iteration : 8800 | Loss : 3.7875176106463693\n",
      "Iteration : 9000 | Loss : 2.5863624281580924\n",
      "Iteration : 9200 | Loss : 2.5517587314096053\n",
      "Iteration : 9400 | Loss : 3.3058233171895717\n",
      "Iteration : 9600 | Loss : 3.4155292560921753\n",
      "Iteration : 9800 | Loss : 4.076968585264295\n",
      "Iteration : 10000 | Loss : 3.9359445336770746\n",
      "Iteration : 10200 | Loss : 3.3215236190379414\n",
      "Iteration : 10400 | Loss : 3.7949835449171445\n",
      "Iteration : 10600 | Loss : 3.1963172067871843\n",
      "Iteration : 10800 | Loss : 4.191663680853041\n",
      "Iteration : 11000 | Loss : 3.9264303274159573\n",
      "Iteration : 11200 | Loss : 3.27010768317016\n",
      "Iteration : 11400 | Loss : 3.7954244867444045\n",
      "Iteration : 11600 | Loss : 3.0227014308714475\n",
      "Iteration : 11800 | Loss : 3.769657946524098\n",
      "Iteration : 12000 | Loss : 4.317232417013736\n",
      "Iteration : 12200 | Loss : 5.678068624023405\n",
      "Iteration : 12400 | Loss : 4.337414871722446\n",
      "Iteration : 12600 | Loss : 4.165276138542157\n",
      "Iteration : 12800 | Loss : 3.5501350101311417\n",
      "Iteration : 13000 | Loss : 3.0350153872750547\n",
      "Iteration : 13200 | Loss : 4.11866602026843\n",
      "Iteration : 13400 | Loss : 3.3979298912591163\n",
      "Iteration : 13600 | Loss : 3.924206292429477\n",
      "Iteration : 13800 | Loss : 4.140252908719136\n",
      "Iteration : 14000 | Loss : 3.962949211820792\n",
      "Iteration : 14200 | Loss : 3.785922158892749\n",
      "Iteration : 14400 | Loss : 3.557681457634434\n",
      "Iteration : 14600 | Loss : 3.6452347907253957\n",
      "Iteration : 14800 | Loss : 3.6041123805029884\n",
      "Iteration : 15000 | Loss : 2.679880222814778\n",
      "Iteration : 15200 | Loss : 2.9072267783548296\n",
      "Iteration : 15400 | Loss : 3.6026595360274563\n",
      "Iteration : 15600 | Loss : 3.6311385632188853\n",
      "Iteration : 15800 | Loss : 2.3903554771993405\n",
      "Iteration : 16000 | Loss : 2.271279424688361\n",
      "Iteration : 16200 | Loss : 2.5833891012253956\n",
      "Iteration : 16400 | Loss : 4.515043104573568\n",
      "Iteration : 16600 | Loss : 3.9133121092183707\n",
      "Iteration : 16800 | Loss : 3.417609115542993\n",
      "Iteration : 17000 | Loss : 2.812186242681153\n",
      "Iteration : 17200 | Loss : 3.957932661533379\n",
      "Iteration : 17400 | Loss : 4.230115421064083\n",
      "Iteration : 17600 | Loss : 3.9483090561416097\n",
      "Iteration : 17800 | Loss : 3.0083499699689606\n",
      "Iteration : 18000 | Loss : 2.9569711159445946\n",
      "Iteration : 18200 | Loss : 4.069889280715701\n",
      "Iteration : 18400 | Loss : 4.555960305792916\n",
      "Iteration : 18600 | Loss : 2.3456776045429284\n",
      "Iteration : 18800 | Loss : 3.097106763457803\n",
      "Iteration : 19000 | Loss : 3.708400190854608\n",
      "Iteration : 19200 | Loss : 3.724013336860653\n",
      "Iteration : 19400 | Loss : 3.112086747836323\n",
      "Iteration : 19600 | Loss : 3.028404856929663\n",
      "Iteration : 19800 | Loss : 2.799053542479347\n",
      "Iteration : 20000 | Loss : 2.2674727783859048\n",
      "Iteration : 20200 | Loss : 2.3995032865413024\n",
      "Iteration : 20400 | Loss : 2.131403797055749\n",
      "Iteration : 20600 | Loss : 3.51437648223159\n",
      "Iteration : 20800 | Loss : 2.202597499521997\n",
      "Iteration : 21000 | Loss : 3.028990974796804\n",
      "Iteration : 21200 | Loss : 3.8206673418906854\n",
      "Iteration : 21400 | Loss : 3.8037922048770425\n",
      "Iteration : 21600 | Loss : 2.421040224020789\n",
      "Iteration : 21800 | Loss : 4.17868086346461\n",
      "Iteration : 22000 | Loss : 3.568524074257855\n",
      "Iteration : 22200 | Loss : 3.309959891512699\n",
      "Iteration : 22400 | Loss : 4.37371885109964\n",
      "Iteration : 22600 | Loss : 2.5588533245448177\n",
      "Iteration : 22800 | Loss : 2.582592511354142\n",
      "Iteration : 23000 | Loss : 3.4498574362009116\n",
      "Iteration : 23200 | Loss : 4.134784301663842\n",
      "Iteration : 23400 | Loss : 3.8935358790190713\n",
      "Iteration : 23600 | Loss : 3.4739501714541445\n",
      "Iteration : 23800 | Loss : 4.11602537657078\n",
      "Iteration : 24000 | Loss : 4.309848331326678\n",
      "Iteration : 24200 | Loss : 3.599528342088946\n",
      "Iteration : 24400 | Loss : 3.7455067416880317\n",
      "Iteration : 24600 | Loss : 4.5559861912278885\n",
      "Iteration : 24800 | Loss : 3.6499048110249346\n",
      "Iteration : 25000 | Loss : 4.099935561479823\n",
      "Iteration : 25200 | Loss : 3.041930299571718\n",
      "Iteration : 25400 | Loss : 3.347995063976126\n",
      "Iteration : 25600 | Loss : 3.8058711713450655\n",
      "Iteration : 25800 | Loss : 3.604203244702872\n",
      "Iteration : 26000 | Loss : 3.727698337122168\n",
      "Iteration : 26200 | Loss : 3.3444756047988884\n",
      "Iteration : 26400 | Loss : 4.353230196675288\n",
      "Iteration : 26600 | Loss : 3.3651158536848653\n",
      "Iteration : 26800 | Loss : 3.2110609296724713\n",
      "Iteration : 27000 | Loss : 4.0476442533813515\n",
      "Iteration : 27200 | Loss : 2.5208546783028423\n",
      "Iteration : 27400 | Loss : 3.8011932556660306\n",
      "Iteration : 27600 | Loss : 3.0512720016014425\n",
      "Iteration : 27800 | Loss : 3.449401149054715\n",
      "Iteration : 28000 | Loss : 3.1875233816843376\n",
      "Iteration : 28200 | Loss : 3.3121874442118964\n",
      "Iteration : 28400 | Loss : 3.345551727336568\n",
      "Iteration : 28600 | Loss : 3.424940958263283\n",
      "Iteration : 28800 | Loss : 2.720922170742768\n",
      "Iteration : 29000 | Loss : 3.900191506254348\n",
      "Iteration : 29200 | Loss : 3.2524058214223563\n",
      "Iteration : 29400 | Loss : 3.2703511562027914\n",
      "Iteration : 29600 | Loss : 3.7174040471969754\n",
      "Iteration : 29800 | Loss : 2.4346971886484665\n",
      "Iteration : 30000 | Loss : 4.067168571661593\n",
      "Iteration : 30200 | Loss : 3.4257042618154423\n",
      "Iteration : 30400 | Loss : 2.69069313769002\n",
      "Iteration : 30600 | Loss : 2.3846995205093475\n",
      "Iteration : 30800 | Loss : 3.521375974457821\n",
      "Iteration : 31000 | Loss : 4.114515992376018\n",
      "Iteration : 31200 | Loss : 3.1177085331559105\n",
      "Iteration : 31400 | Loss : 4.508714419336434\n",
      "Iteration : 31600 | Loss : 5.004794111624463\n",
      "Iteration : 31800 | Loss : 3.2887367176929203\n",
      "Iteration : 32000 | Loss : 3.6660789267997\n",
      "Iteration : 32200 | Loss : 3.862392417441415\n",
      "Iteration : 32400 | Loss : 2.7373384903470876\n",
      "Iteration : 32600 | Loss : 4.226960891372344\n",
      "Iteration : 32800 | Loss : 4.326475258559055\n",
      "Iteration : 33000 | Loss : 2.4329751347897197\n",
      "Iteration : 33200 | Loss : 3.464001425894125\n",
      "Iteration : 33400 | Loss : 3.9050736189035264\n",
      "Iteration : 33600 | Loss : 3.4912957837097567\n",
      "Iteration : 33800 | Loss : 3.3767719507511345\n",
      "Iteration : 34000 | Loss : 3.0976656858667027\n",
      "Iteration : 34200 | Loss : 2.5179641593659836\n",
      "Iteration : 34400 | Loss : 4.733522656721909\n",
      "Iteration : 34600 | Loss : 3.822920364167653\n",
      "Iteration : 34800 | Loss : 4.633991343882606\n",
      "Iteration : 35000 | Loss : 4.380158181000017\n",
      "Iteration : 35200 | Loss : 3.5099462636358183\n",
      "Iteration : 35400 | Loss : 2.9974020001085693\n",
      "Iteration : 35600 | Loss : 3.941085861677299\n",
      "Iteration : 35800 | Loss : 3.579925328058323\n",
      "Iteration : 36000 | Loss : 3.900688556819347\n",
      "Iteration : 36200 | Loss : 4.333890088317939\n",
      "Iteration : 36400 | Loss : 4.324554925657468\n",
      "Iteration : 36600 | Loss : 3.101112563339302\n",
      "Iteration : 36800 | Loss : 4.253381667604963\n",
      "Iteration : 37000 | Loss : 3.3766172900197424\n",
      "Iteration : 37200 | Loss : 4.96392685581269\n",
      "Iteration : 37400 | Loss : 3.502249994272963\n",
      "Iteration : 37600 | Loss : 4.1110435371308265\n",
      "Iteration : 37800 | Loss : 3.2227867343619323\n",
      "Iteration : 38000 | Loss : 3.8274440872162985\n",
      "Iteration : 38200 | Loss : 3.0565981586485815\n",
      "Iteration : 38400 | Loss : 4.0559247966364484\n",
      "Iteration : 38600 | Loss : 4.5746227750372\n",
      "Iteration : 38800 | Loss : 2.945121671916934\n",
      "Iteration : 39000 | Loss : 2.8371166974628284\n",
      "Iteration : 39200 | Loss : 3.708347928771123\n",
      "Iteration : 39400 | Loss : 4.754070781638346\n",
      "Iteration : 39600 | Loss : 3.2989184711291437\n",
      "Iteration : 39800 | Loss : 3.5320919974607947\n",
      "Iteration : 40000 | Loss : 2.208397314788884\n",
      "Iteration : 40200 | Loss : 3.88721480267512\n",
      "Iteration : 40400 | Loss : 2.6249938154104484\n",
      "Iteration : 40600 | Loss : 3.72207966246174\n",
      "Iteration : 40800 | Loss : 2.7882044568561355\n",
      "Iteration : 41000 | Loss : 4.29672575768529\n",
      "Iteration : 41200 | Loss : 3.8350664709345317\n",
      "Iteration : 41400 | Loss : 4.829025628870264\n",
      "Iteration : 41600 | Loss : 3.8243968546422202\n",
      "Iteration : 41800 | Loss : 2.3292695958069043\n",
      "Iteration : 42000 | Loss : 3.6038739273716587\n",
      "Iteration : 42200 | Loss : 4.618416274717001\n",
      "Iteration : 42400 | Loss : 3.728700421531483\n",
      "Iteration : 42600 | Loss : 2.3038249406485005\n",
      "Iteration : 42800 | Loss : 4.035984007400748\n",
      "Iteration : 43000 | Loss : 3.0333089027052513\n",
      "Iteration : 43200 | Loss : 3.3832132218396693\n",
      "Iteration : 43400 | Loss : 3.3116033651320613\n",
      "Iteration : 43600 | Loss : 3.8368506571504852\n",
      "Iteration : 43800 | Loss : 4.474732307553362\n",
      "Iteration : 44000 | Loss : 3.785117271735576\n",
      "Iteration : 44200 | Loss : 2.400672935231026\n",
      "Iteration : 44400 | Loss : 2.9486194833459134\n",
      "Iteration : 44600 | Loss : 4.7703508053520975\n",
      "Iteration : 44800 | Loss : 3.313734248132227\n",
      "Iteration : 45000 | Loss : 3.249213922969455\n",
      "Iteration : 45200 | Loss : 3.8222655928216556\n",
      "Iteration : 45400 | Loss : 4.400111297802595\n",
      "Iteration : 45600 | Loss : 3.0829336557790468\n",
      "Iteration : 45800 | Loss : 3.03244002326513\n",
      "Iteration : 46000 | Loss : 3.208598022022187\n",
      "Iteration : 46200 | Loss : 3.5976800339170056\n",
      "Iteration : 46400 | Loss : 3.59842398761604\n",
      "Iteration : 46600 | Loss : 3.5497987876587573\n",
      "Iteration : 46800 | Loss : 4.609586530483741\n",
      "Iteration : 47000 | Loss : 4.150025206389259\n",
      "Iteration : 47200 | Loss : 3.9569658027914327\n",
      "Iteration : 47400 | Loss : 3.7376111582977964\n",
      "Iteration : 47600 | Loss : 3.832681872142307\n",
      "Iteration : 47800 | Loss : 4.711504968451044\n",
      "Iteration : 48000 | Loss : 3.95897468093773\n",
      "Iteration : 48200 | Loss : 4.508264380114419\n",
      "Iteration : 48400 | Loss : 3.7837982386443607\n",
      "Iteration : 48600 | Loss : 4.559401492249782\n",
      "Iteration : 48800 | Loss : 4.448057669764742\n",
      "Iteration : 49000 | Loss : 3.8669085151005285\n",
      "Iteration : 49200 | Loss : 3.4442619268564796\n",
      "Iteration : 49400 | Loss : 3.447324031811641\n",
      "Iteration : 49600 | Loss : 3.3568778653165716\n",
      "Iteration : 49800 | Loss : 4.651253198311865\n",
      "Iteration : 50000 | Loss : 2.4790640329478193\n",
      "Iteration : 50200 | Loss : 2.9133001386660125\n",
      "Iteration : 50400 | Loss : 5.063300665445403\n",
      "Iteration : 50600 | Loss : 4.580882912454248\n",
      "Iteration : 50800 | Loss : 3.9550889743881554\n",
      "Iteration : 51000 | Loss : 3.0215495351505637\n",
      "Iteration : 51200 | Loss : 3.46986554952053\n",
      "Iteration : 51400 | Loss : 3.5135590299697546\n",
      "Iteration : 51600 | Loss : 4.511788492640575\n",
      "Iteration : 51800 | Loss : 3.573558829881127\n",
      "Iteration : 52000 | Loss : 3.525835253614249\n",
      "Iteration : 52200 | Loss : 3.244193713504528\n",
      "Iteration : 52400 | Loss : 3.178817224042827\n",
      "Iteration : 52600 | Loss : 3.0446864020048183\n",
      "Iteration : 52800 | Loss : 3.633336825839499\n",
      "Iteration : 53000 | Loss : 3.844474591259086\n",
      "Iteration : 53200 | Loss : 2.7650841308547136\n",
      "Iteration : 53400 | Loss : 3.5925395840973793\n",
      "Iteration : 53600 | Loss : 3.0894539387521154\n",
      "Iteration : 53800 | Loss : 3.4025944961806167\n",
      "Iteration : 54000 | Loss : 3.9842081108206715\n",
      "Iteration : 54200 | Loss : 3.691920694478577\n",
      "Iteration : 54400 | Loss : 2.914011336847984\n",
      "Iteration : 54600 | Loss : 2.347639380813438\n",
      "Iteration : 54800 | Loss : 2.716565860231808\n",
      "Iteration : 55000 | Loss : 3.095455751706082\n",
      "Iteration : 55200 | Loss : 3.7542365442805035\n",
      "Iteration : 55400 | Loss : 3.970513015499583\n",
      "Iteration : 55600 | Loss : 3.1509816380785596\n",
      "Iteration : 55800 | Loss : 3.082514965455553\n",
      "Iteration : 56000 | Loss : 4.37268051472798\n",
      "Iteration : 56200 | Loss : 3.893784785388065\n",
      "Iteration : 56400 | Loss : 5.02715115547475\n",
      "Iteration : 56600 | Loss : 4.562522691187268\n",
      "Iteration : 56800 | Loss : 3.845340023462961\n",
      "Iteration : 57000 | Loss : 3.273124762215924\n",
      "Iteration : 57200 | Loss : 2.8312553302957184\n",
      "Iteration : 57400 | Loss : 4.520582246689915\n",
      "Iteration : 57600 | Loss : 2.8262833609472127\n",
      "Iteration : 57800 | Loss : 4.197301592462951\n",
      "Iteration : 58000 | Loss : 4.431896234375104\n",
      "Iteration : 58200 | Loss : 3.710557722243449\n",
      "Iteration : 58400 | Loss : 4.191563780600771\n",
      "Iteration : 58600 | Loss : 3.796388337937144\n",
      "Iteration : 58800 | Loss : 3.6895523126760317\n",
      "Iteration : 59000 | Loss : 4.215451852869218\n",
      "Iteration : 59200 | Loss : 4.730054428835457\n",
      "Iteration : 59400 | Loss : 3.7146350265647343\n",
      "Iteration : 59600 | Loss : 3.920299444120631\n",
      "Iteration : 59800 | Loss : 3.9002176168709672\n",
      "Iteration : 60000 | Loss : 5.072911785437222\n",
      "Iteration : 60200 | Loss : 4.678690345771148\n",
      "Iteration : 60400 | Loss : 5.007244498368124\n",
      "Iteration : 60600 | Loss : 3.9332529250341546\n",
      "Iteration : 60800 | Loss : 5.578256015537649\n",
      "Iteration : 61000 | Loss : 2.93779726695813\n",
      "Iteration : 61200 | Loss : 4.664138883536209\n",
      "Iteration : 61400 | Loss : 4.456868817003659\n",
      "Iteration : 61600 | Loss : 3.474059575097896\n",
      "Iteration : 61800 | Loss : 3.7368721116645167\n",
      "Iteration : 62000 | Loss : 3.0988293293523754\n",
      "Iteration : 62200 | Loss : 3.2104681971785483\n",
      "Iteration : 62400 | Loss : 3.023469755264446\n",
      "Iteration : 62600 | Loss : 4.110801045624964\n",
      "Iteration : 62800 | Loss : 2.6670114254057626\n",
      "Iteration : 63000 | Loss : 3.404805978462992\n",
      "Iteration : 63200 | Loss : 4.702122384307281\n",
      "Iteration : 63400 | Loss : 3.8321578992937604\n",
      "Iteration : 63600 | Loss : 3.537775885186844\n",
      "Iteration : 63800 | Loss : 2.730511929612185\n",
      "Iteration : 64000 | Loss : 3.4734364139415623\n",
      "Iteration : 64200 | Loss : 4.033823441222181\n",
      "Iteration : 64400 | Loss : 3.560279193217989\n",
      "Iteration : 64600 | Loss : 4.361757191272308\n",
      "Iteration : 64800 | Loss : 4.099070677410355\n",
      "Iteration : 65000 | Loss : 3.0028691128915415\n",
      "Iteration : 65200 | Loss : 4.5216773464331235\n",
      "Iteration : 65400 | Loss : 3.4678813193551163\n",
      "Iteration : 65600 | Loss : 2.900680340796158\n",
      "Iteration : 65800 | Loss : 4.067874815103376\n",
      "Iteration : 66000 | Loss : 3.7592853138223083\n",
      "Iteration : 66200 | Loss : 4.065552848667728\n",
      "Iteration : 66400 | Loss : 3.0352575893631464\n",
      "Iteration : 66600 | Loss : 3.4646552777686934\n",
      "Iteration : 66800 | Loss : 3.671566506120297\n",
      "Iteration : 67000 | Loss : 4.093035087764276\n",
      "Iteration : 67200 | Loss : 3.3732159346919763\n",
      "Iteration : 67400 | Loss : 3.496740969379328\n",
      "Iteration : 67600 | Loss : 3.2858784312373763\n",
      "Iteration : 67800 | Loss : 4.623673470563222\n",
      "Iteration : 68000 | Loss : 3.5632984118976054\n",
      "Iteration : 68200 | Loss : 4.466133825679439\n",
      "Iteration : 68400 | Loss : 2.143094701849893\n",
      "Iteration : 68600 | Loss : 2.8801197847326407\n",
      "Iteration : 68800 | Loss : 3.681674680754375\n",
      "Iteration : 69000 | Loss : 5.226804080427663\n",
      "Iteration : 69200 | Loss : 4.565291620197493\n",
      "Iteration : 69400 | Loss : 3.605553255347549\n",
      "Iteration : 69600 | Loss : 4.453661760036939\n",
      "Iteration : 69800 | Loss : 3.8523237092183855\n",
      "Iteration : 70000 | Loss : 3.487052180734863\n",
      "Iteration : 70200 | Loss : 3.0906019494078913\n",
      "Iteration : 70400 | Loss : 3.875738876047309\n",
      "Iteration : 70600 | Loss : 4.700756129867843\n",
      "Iteration : 70800 | Loss : 4.5635098799008755\n",
      "Iteration : 71000 | Loss : 3.5020140396876327\n",
      "Iteration : 71200 | Loss : 4.150698941730547\n",
      "Iteration : 71400 | Loss : 2.7784179436850596\n",
      "Iteration : 71600 | Loss : 3.9093905205794774\n",
      "Iteration : 71800 | Loss : 3.816736758846023\n",
      "Iteration : 72000 | Loss : 3.5120454422312735\n",
      "Iteration : 72200 | Loss : 3.9850621605502354\n",
      "Iteration : 72400 | Loss : 2.4988753464356415\n",
      "Iteration : 72600 | Loss : 3.9178799850028008\n",
      "Iteration : 72800 | Loss : 2.559295629923137\n",
      "Iteration : 73000 | Loss : 3.7978772373404843\n",
      "Iteration : 73200 | Loss : 4.270385374172384\n",
      "Iteration : 73400 | Loss : 2.885951769189403\n",
      "Iteration : 73600 | Loss : 3.756317188290306\n",
      "Iteration : 73800 | Loss : 2.8727737684376082\n",
      "Iteration : 74000 | Loss : 3.7724600064690685\n",
      "Iteration : 74200 | Loss : 2.907840347680703\n",
      "Iteration : 74400 | Loss : 3.296279536709933\n",
      "Iteration : 74600 | Loss : 4.238296225049817\n",
      "Iteration : 74800 | Loss : 4.041562270918353\n",
      "Iteration : 75000 | Loss : 3.057885972944539\n",
      "Iteration : 75200 | Loss : 3.0053968980675734\n",
      "Iteration : 75400 | Loss : 2.7084456569263464\n",
      "Iteration : 75600 | Loss : 4.361237776630128\n",
      "Iteration : 75800 | Loss : 2.307362542441907\n",
      "Iteration : 76000 | Loss : 4.01483420325795\n",
      "Iteration : 76200 | Loss : 3.4188835340361585\n",
      "Iteration : 76400 | Loss : 3.3570904503849155\n",
      "Iteration : 76600 | Loss : 3.2793591644830413\n",
      "Iteration : 76800 | Loss : 3.6800788805073594\n",
      "Iteration : 77000 | Loss : 4.050957449964181\n",
      "Iteration : 77200 | Loss : 4.275045197470517\n",
      "Iteration : 77400 | Loss : 3.9850677758670665\n",
      "Iteration : 77600 | Loss : 3.5072919326660505\n",
      "Iteration : 77800 | Loss : 5.153767398649543\n",
      "Iteration : 78000 | Loss : 3.334290907212941\n",
      "Iteration : 78200 | Loss : 3.9872789358224088\n",
      "Iteration : 78400 | Loss : 4.249035332639771\n",
      "Iteration : 78600 | Loss : 4.063615836520582\n",
      "Iteration : 78800 | Loss : 2.391113337154767\n",
      "Iteration : 79000 | Loss : 2.670317570839476\n",
      "Iteration : 79200 | Loss : 3.2258980764472502\n",
      "Iteration : 79400 | Loss : 2.386301962246808\n",
      "Iteration : 79600 | Loss : 3.0269200501165985\n",
      "Iteration : 79800 | Loss : 2.181623786827391\n",
      "Iteration : 80000 | Loss : 2.8121337171452434\n",
      "Iteration : 80200 | Loss : 4.961806569299446\n",
      "Iteration : 80400 | Loss : 3.409889914894832\n",
      "Iteration : 80600 | Loss : 4.056007255053311\n",
      "Iteration : 80800 | Loss : 2.470506173677086\n",
      "Iteration : 81000 | Loss : 3.763798084746861\n",
      "Iteration : 81200 | Loss : 3.8491465770328555\n",
      "Iteration : 81400 | Loss : 4.926351537373998\n",
      "Iteration : 81600 | Loss : 3.5210877278903774\n",
      "Iteration : 81800 | Loss : 3.857063966356862\n",
      "Iteration : 82000 | Loss : 3.0608100450910807\n",
      "Iteration : 82200 | Loss : 3.9794089827376453\n",
      "Iteration : 82400 | Loss : 3.3230547857626953\n",
      "Iteration : 82600 | Loss : 2.8933223661384937\n",
      "Iteration : 82800 | Loss : 3.195406187173302\n",
      "Iteration : 83000 | Loss : 3.236316240014144\n",
      "Iteration : 83200 | Loss : 4.685821171482777\n",
      "Iteration : 83400 | Loss : 3.7431186940799095\n",
      "Iteration : 83600 | Loss : 3.681817740544878\n",
      "Iteration : 83800 | Loss : 2.4505902872472363\n",
      "Iteration : 84000 | Loss : 4.525751063299431\n",
      "Iteration : 84200 | Loss : 3.4804746694739586\n",
      "Iteration : 84400 | Loss : 3.4041393788107444\n",
      "Iteration : 84600 | Loss : 5.113473529402414\n",
      "Iteration : 84800 | Loss : 3.126537791194453\n",
      "Iteration : 85000 | Loss : 4.799179545081605\n",
      "Iteration : 85200 | Loss : 3.4342036195173784\n",
      "Iteration : 85400 | Loss : 4.138332341173767\n",
      "Iteration : 85600 | Loss : 4.397617613816466\n",
      "Iteration : 85800 | Loss : 4.101313390401971\n",
      "Iteration : 86000 | Loss : 3.3188470483926187\n",
      "Iteration : 86200 | Loss : 4.272804692981966\n",
      "Iteration : 86400 | Loss : 4.053507384433783\n",
      "Iteration : 86600 | Loss : 3.7176143648425635\n",
      "Iteration : 86800 | Loss : 3.5224778021596115\n",
      "Iteration : 87000 | Loss : 2.623073376811134\n",
      "Iteration : 87200 | Loss : 4.567117174844731\n",
      "Iteration : 87400 | Loss : 3.58464846697132\n",
      "Iteration : 87600 | Loss : 2.758639957437692\n",
      "Iteration : 87800 | Loss : 3.3068407319361817\n",
      "Iteration : 88000 | Loss : 4.442209192414198\n",
      "Iteration : 88200 | Loss : 3.662711593687574\n",
      "Iteration : 88400 | Loss : 3.7474447879613577\n",
      "Iteration : 88600 | Loss : 4.1169480095698\n",
      "Iteration : 88800 | Loss : 4.4053334042051535\n",
      "Iteration : 89000 | Loss : 3.2219718751266924\n",
      "Iteration : 89200 | Loss : 4.6563855627870865\n",
      "Iteration : 89400 | Loss : 4.410677035913945\n",
      "Iteration : 89600 | Loss : 4.369770460769753\n",
      "Iteration : 89800 | Loss : 2.2870419447565418\n",
      "Iteration : 90000 | Loss : 3.347358099914107\n",
      "Iteration : 90200 | Loss : 2.3448796291666003\n",
      "Iteration : 90400 | Loss : 3.3003081169853905\n",
      "Iteration : 90600 | Loss : 4.256009027500524\n",
      "Iteration : 90800 | Loss : 3.0814935607961536\n",
      "Iteration : 91000 | Loss : 4.018106549839165\n",
      "Iteration : 91200 | Loss : 3.3796016359917322\n",
      "Iteration : 91400 | Loss : 3.7957950537385488\n",
      "Iteration : 91600 | Loss : 3.6195738955880215\n",
      "Iteration : 91800 | Loss : 4.191582813674639\n",
      "Iteration : 92000 | Loss : 3.378190996410102\n",
      "Iteration : 92200 | Loss : 4.416628291283841\n",
      "Iteration : 92400 | Loss : 3.545127024558609\n",
      "Iteration : 92600 | Loss : 5.008107301796132\n",
      "Iteration : 92800 | Loss : 3.4587811434024283\n",
      "Iteration : 93000 | Loss : 4.138494561167738\n",
      "Iteration : 93200 | Loss : 2.9477527931879073\n",
      "Iteration : 93400 | Loss : 5.365539099261955\n",
      "Iteration : 93600 | Loss : 4.05715585677005\n",
      "Iteration : 93800 | Loss : 3.5763136654401038\n",
      "Iteration : 94000 | Loss : 3.444425235166984\n",
      "Iteration : 94200 | Loss : 3.748128362170393\n",
      "Iteration : 94400 | Loss : 2.496784421245727\n",
      "Iteration : 94600 | Loss : 4.101079214840032\n",
      "Iteration : 94800 | Loss : 5.5691974591700015\n",
      "Iteration : 95000 | Loss : 4.226094230555571\n",
      "Iteration : 95200 | Loss : 4.1829611423090265\n",
      "Iteration : 95400 | Loss : 3.9212975340999954\n",
      "Iteration : 95600 | Loss : 5.34874331378334\n",
      "Iteration : 95800 | Loss : 3.6486119274268205\n",
      "Iteration : 96000 | Loss : 3.6344209913259817\n",
      "Iteration : 96200 | Loss : 4.275134874487653\n",
      "Iteration : 96400 | Loss : 4.7720280593214435\n",
      "Iteration : 96600 | Loss : 4.813821216837761\n",
      "Iteration : 96800 | Loss : 3.192968921386897\n",
      "Iteration : 97000 | Loss : 2.840478447826593\n",
      "Iteration : 97200 | Loss : 3.807955959081526\n",
      "Iteration : 97400 | Loss : 2.8409771295901907\n",
      "Iteration : 97600 | Loss : 2.921377634116703\n",
      "Iteration : 97800 | Loss : 3.2786051545795187\n",
      "Iteration : 98000 | Loss : 3.4721989809386073\n",
      "Iteration : 98200 | Loss : 3.078535719891048\n",
      "Iteration : 98400 | Loss : 3.838169168064087\n",
      "Iteration : 98600 | Loss : 4.24907644395499\n",
      "Iteration : 98800 | Loss : 3.007423409640083\n",
      "Iteration : 99000 | Loss : 3.7522259842595296\n",
      "Iteration : 99200 | Loss : 3.245654492951921\n",
      "Iteration : 99400 | Loss : 3.061534954092339\n",
      "Iteration : 99600 | Loss : 3.4593603259496377\n",
      "Iteration : 99800 | Loss : 3.2504899380204293\n",
      "Iteration : 100000 | Loss : 4.223459227017775\n",
      "Iteration : 100200 | Loss : 4.0376056547163\n",
      "Iteration : 100400 | Loss : 3.9996249892991287\n",
      "Iteration : 100600 | Loss : 4.0008431949569445\n",
      "Iteration : 100800 | Loss : 4.033733390406076\n",
      "Iteration : 101000 | Loss : 4.777973244797532\n",
      "Iteration : 101200 | Loss : 2.3911208876409353\n",
      "Iteration : 101400 | Loss : 4.51680383830028\n",
      "Iteration : 101600 | Loss : 2.2954968478008686\n",
      "Iteration : 101800 | Loss : 2.065026077308402\n",
      "Iteration : 102000 | Loss : 2.6178124225008683\n",
      "Iteration : 102200 | Loss : 3.708747313843798\n",
      "Iteration : 102400 | Loss : 4.373575618826399\n",
      "Iteration : 102600 | Loss : 4.251213742932017\n",
      "Iteration : 102800 | Loss : 4.474952200426737\n",
      "Iteration : 103000 | Loss : 4.116757083949197\n",
      "Iteration : 103200 | Loss : 4.009135324538078\n",
      "Iteration : 103400 | Loss : 4.766079438073897\n",
      "Iteration : 103600 | Loss : 3.8109860126058\n",
      "Iteration : 103800 | Loss : 2.948366841553904\n",
      "Iteration : 104000 | Loss : 4.272961950303194\n",
      "Iteration : 104200 | Loss : 3.282437303622417\n",
      "Iteration : 104400 | Loss : 4.694171573081123\n",
      "Iteration : 104600 | Loss : 3.8164853768743137\n",
      "Iteration : 104800 | Loss : 3.569499298140509\n",
      "Iteration : 105000 | Loss : 3.1039307533915723\n",
      "Iteration : 105200 | Loss : 4.938341059679078\n",
      "Iteration : 105400 | Loss : 3.273456234311302\n",
      "Iteration : 105600 | Loss : 3.7473113476713613\n",
      "Iteration : 105800 | Loss : 2.7024071540161323\n",
      "Iteration : 106000 | Loss : 4.3485344591167685\n",
      "Iteration : 106200 | Loss : 2.61809953217256\n",
      "Iteration : 106400 | Loss : 4.61095372728982\n",
      "Iteration : 106600 | Loss : 2.488111034474009\n",
      "Iteration : 106800 | Loss : 3.805968391903207\n",
      "Iteration : 107000 | Loss : 3.0958134057233813\n",
      "Iteration : 107200 | Loss : 4.125474900914975\n",
      "Iteration : 107400 | Loss : 3.60549282849484\n",
      "Iteration : 107600 | Loss : 4.0737395869454\n",
      "Iteration : 107800 | Loss : 4.4331806890772265\n",
      "Iteration : 108000 | Loss : 3.035280445243964\n",
      "Iteration : 108200 | Loss : 5.102715668087145\n",
      "Iteration : 108400 | Loss : 2.300007482205412\n",
      "Iteration : 108600 | Loss : 3.6327685861798544\n",
      "Iteration : 108800 | Loss : 3.6589012283780145\n",
      "Iteration : 109000 | Loss : 3.728549093019204\n",
      "Iteration : 109200 | Loss : 3.3206647018766793\n",
      "Iteration : 109400 | Loss : 3.006886637050767\n",
      "Iteration : 109600 | Loss : 4.556971222802117\n",
      "Iteration : 109800 | Loss : 2.8479326565678784\n",
      "Iteration : 110000 | Loss : 3.2731746260432373\n",
      "Iteration : 110200 | Loss : 4.009276763862429\n",
      "Iteration : 110400 | Loss : 4.621241811594416\n",
      "Iteration : 110600 | Loss : 4.179452505625668\n",
      "Iteration : 110800 | Loss : 3.3116075133126714\n",
      "Iteration : 111000 | Loss : 2.558611347814885\n",
      "Iteration : 111200 | Loss : 3.8735625614240954\n",
      "Iteration : 111400 | Loss : 2.1710939312042803\n",
      "Iteration : 111600 | Loss : 4.337476271966061\n",
      "Iteration : 111800 | Loss : 3.3959081187270206\n",
      "Iteration : 112000 | Loss : 4.131011175638447\n",
      "Iteration : 112200 | Loss : 4.061191769943158\n",
      "Iteration : 112400 | Loss : 4.059137831285365\n",
      "Iteration : 112600 | Loss : 2.95180300113532\n",
      "Iteration : 112800 | Loss : 4.920673606889687\n",
      "Iteration : 113000 | Loss : 4.722672156697013\n",
      "Iteration : 113200 | Loss : 4.659423431742342\n",
      "Iteration : 113400 | Loss : 3.4054082646254145\n",
      "Iteration : 113600 | Loss : 3.3995011119611886\n",
      "Iteration : 113800 | Loss : 3.4836016544532358\n",
      "Iteration : 114000 | Loss : 4.21327368857701\n",
      "Iteration : 114200 | Loss : 3.1319313269616784\n",
      "Iteration : 114400 | Loss : 4.663603181161556\n",
      "Iteration : 114600 | Loss : 5.444497213223135\n",
      "Iteration : 114800 | Loss : 3.821674410144228\n",
      "Iteration : 115000 | Loss : 5.601560547222912\n",
      "Iteration : 115200 | Loss : 3.8124388904540645\n",
      "Iteration : 115400 | Loss : 4.108580913030759\n",
      "Iteration : 115600 | Loss : 2.9718426829806366\n",
      "Iteration : 115800 | Loss : 3.49320084398432\n",
      "Iteration : 116000 | Loss : 4.352247858569915\n",
      "Iteration : 116200 | Loss : 3.177219225062328\n",
      "Iteration : 116400 | Loss : 3.7514569799046584\n",
      "Iteration : 116600 | Loss : 4.357393809552578\n",
      "Iteration : 116800 | Loss : 4.317249350747535\n",
      "Iteration : 117000 | Loss : 4.249852633422346\n",
      "Iteration : 117200 | Loss : 4.128104530226776\n",
      "Iteration : 117400 | Loss : 3.7675097932120067\n",
      "Iteration : 117600 | Loss : 5.6000217061553785\n",
      "Iteration : 117800 | Loss : 3.894753457313322\n",
      "Iteration : 118000 | Loss : 4.15690404475513\n",
      "Iteration : 118200 | Loss : 4.454796798147482\n",
      "Iteration : 118400 | Loss : 3.712305121835361\n",
      "Iteration : 118600 | Loss : 3.603613382597625\n",
      "Iteration : 118800 | Loss : 3.263988815148211\n",
      "Iteration : 119000 | Loss : 4.042447933543952\n",
      "Iteration : 119200 | Loss : 2.7801360771959645\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "for _ in range(epoch):\n",
    "    model2.train(learning_rate = 1e-4, show_loss_every = 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "olivia\n",
      "ava\n",
      "isabella\n",
      "sophia\n",
      "charlotte\n",
      "mia\n",
      "amelia\n",
      "harper\n",
      "evelyn\n",
      "abigail\n",
      "emily\n",
      "elizabeth\n",
      "mila\n",
      "ella\n",
      "----------------------------------------\n",
      "z\n",
      "ir\n",
      "ynaeenr\n",
      "n\n",
      "rsarmsseoiyrsls\n",
      "larerae\n",
      "miieinae\n",
      "\n",
      "y\n",
      "nl\n",
      "anio\n",
      "anooi\n",
      "olneninierryrrae\n",
      "\n",
      "n\n",
      "m\n",
      "isi\n",
      "r\n",
      "soren\n",
      "rr\n"
     ]
    }
   ],
   "source": [
    "print(model2.data[:100])\n",
    "print('-'*40)\n",
    "print(model2.sample(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fae8883b610>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(model2.iter_record, model2.loss_record)\n",
    "\n",
    "smoothed_loss = [model2.loss_record[0]]\n",
    "loss_mem = model2.loss_record[0]\n",
    "alpha = 0.9\n",
    "for loss in model2.loss_record[1:]:\n",
    "    loss_mem = alpha * loss_mem + (1 - alpha) * loss\n",
    "    smoothed_loss.append(loss_mem)\n",
    "\n",
    "plt.plot(model2.iter_record, smoothed_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02496147  0.04442661 -0.05515717  0.33349057]\n",
      " [-0.37570743 -0.30966412 -0.31637115  0.43567264]\n",
      " [ 0.05383875  0.04090016  0.02574919  0.15932472]\n",
      " [-0.1511928  -0.0966722  -0.13375202  0.01244092]]\n",
      "[[-0.5095012   1.00041911 -0.56844687]\n",
      " [-0.44606827  1.01067685 -0.57662515]\n",
      " [-0.45847428  1.01073574 -0.58339314]\n",
      " [ 0.26484994 -0.9925311   0.58384647]]\n"
     ]
    }
   ],
   "source": [
    "print(model2.Whh)\n",
    "print(model2.Why)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34138449237715834"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.abs(np.linalg.eig(model2.Whh)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 7 characters, 3 unique.\n",
      "One epoch = 0.175 iterations\n",
      "==================================================\n",
      "-0.004565937583578261 -0.0045644039113151855\n"
     ]
    }
   ],
   "source": [
    "model = RNN(data, hidden_size=100, seq_length=40, reg=0)\n",
    "x = model.X[0:1]\n",
    "label = model.label[0:1]\n",
    "h0 = np.zeros((1, model.Whh.shape[0]))\n",
    "h, y = model.forward(x, h0)\n",
    "dWhh, dWxh, dWhy, dbh, dby, dhprev = model.backward(h0, h, x, y, h0, label)\n",
    "\n",
    "eps = 1e-3\n",
    "model.bh[0, 0] += eps * 0.5\n",
    "\n",
    "_, y2 = model.forward(x, h0)\n",
    "err2 = model.cross_entropy(y2, label)\n",
    "\n",
    "model.bh[0, 0] -= eps \n",
    "_, y1 = model.forward(x, h0)\n",
    "err1 = model.cross_entropy(y1, label)\n",
    "\n",
    "print((err2 - err1) / eps, dbh[0, 0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "[2 2 0 1 2 2 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'emmaemma'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(model2.X[:10])\n",
    "print(model2.label[:10])\n",
    "model2.data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 69 characters, 6 unique.\n",
      "One epoch = 6.900 iterations\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "data = '<hello>' * 10\n",
    "#np.random.seed(1234)\n",
    "model = RNN(data, hidden_size=5, seq_length=10)\n",
    "\n",
    "\n",
    "h1, y1 = model.forward(model.X[0], np.zeros((1,model.Whh.shape[0])))\n",
    "h2, y2 = model.forward(model.X[1], h1)\n",
    "h3, y3 = model.forward(model.X[2], h2)\n",
    "h4, y4 = model.forward(model.X[3], h3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0 | Loss : 1.7917740824834596\n",
      "Iteration : 100 | Loss : 0.4014817436909208\n",
      "Iteration : 200 | Loss : 0.070035541605897\n",
      "Iteration : 300 | Loss : 0.15016926911599698\n",
      "Iteration : 400 | Loss : 0.002422179201722582\n",
      "Iteration : 500 | Loss : 0.0005600406082354103\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    model.train(learning_rate=1e-3, show_loss_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.01484830e-05 9.92878106e-04 1.82012767e-01 8.15768174e-01\n",
      "  1.19492330e-03 1.10888101e-06]]\n",
      "[[1.46423102e-09 2.77893605e-09 9.92107603e-01 7.69480793e-05\n",
      "  7.81544327e-03 1.48761725e-09]]\n",
      "[[5.81390150e-13 5.22543236e-12 6.00845968e-06 7.40736011e-11\n",
      "  9.99993528e-01 4.63905519e-07]]\n",
      "[[1.54814060e-08 7.16664966e-08 2.66608504e-05 4.63128054e-09\n",
      "  9.99096108e-01 8.77139582e-04]]\n"
     ]
    }
   ],
   "source": [
    "h1, y1 = model.forward(model.X[0], np.zeros((1,model.Whh.shape[0])))\n",
    "h2, y2 = model.forward(model.X[1], h1)\n",
    "h3, y3 = model.forward(model.X[2], h2)\n",
    "h4, y4 = model.forward(model.X[3], h3)\n",
    "\n",
    "print(model.softmax(y1))\n",
    "print(model.softmax(y2))\n",
    "print(model.softmax(y3))\n",
    "print(model.softmax(y4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<hello><hello><hello><hello><hello><hello><hello><hello><hello><hello>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 e\n",
      "1 h\n",
      "2 l\n",
      "3 o\n"
     ]
    }
   ],
   "source": [
    "for i in range(model.vocab_size):\n",
    "    print(i, model.itos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<hello><hello><hello><hello><hello><hello><hello><hello><hello><hello><hello><hello><hello><hello><he'"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(model.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.72930537, -0.59057829, -0.48102029,  0.87794895,  0.5678633 ,\n",
       "         0.34575078,  0.98417853, -0.021427  ,  0.20977749,  0.56131748,\n",
       "         0.74220912,  0.44643582, -0.71786952,  0.69220652,  0.69141017,\n",
       "        -0.89769963, -0.84473366, -0.85919479, -0.99956936,  0.72397639,\n",
       "        -0.99706999,  0.71739544,  0.42945244, -0.77483041,  0.86294879,\n",
       "         0.88211953,  0.34463054,  0.72955704, -0.53508888, -0.84294012,\n",
       "         0.6779615 ,  0.64225493, -0.84583497,  0.69759273, -0.8895467 ,\n",
       "        -0.02620606, -0.99999999, -0.91935093, -0.51296011,  0.79377445,\n",
       "         0.47999043,  0.35940789,  0.70580092, -0.78101248, -0.53586927,\n",
       "        -0.87777636, -0.7695683 ,  0.67899763, -0.52635927, -0.77808618]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.h_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
